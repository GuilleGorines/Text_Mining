{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports previos\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import networkx as nx\n",
    "\n",
    "from Bio import Medline\n",
    "from Bio import Entrez\n",
    "\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones necesarias:\n",
    "\n",
    "def key_from_value(dict, query):\n",
    "    for key, values in dict.items():\n",
    "        for synonym in values:\n",
    "            if query == synonym:\n",
    "                return key\n",
    "\n",
    "def back_to_value(dict, query):\n",
    "    for keys,values in dict.items():\n",
    "        for key in keys:\n",
    "            if query == key:\n",
    "                return value[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database para enfermedades"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLAN RAW PARA ENFERMEDADES**: en Disease Ontology, hay una lista<sup>1</sup> que contiene un ID de la enfermedad, con el id, nombre de la enfermedad y sinónimos. La idea es generar un diccionario  {key: ID y values: nombres de la enfermedad} y si se encuentra en un abstract la enfermedad (es decir, el value), cambiarlo por el ID (la key)\n",
    " \n",
    " <sup>1</sup>https://github.com/DiseaseOntology/HumanDiseaseOntology/blob/main/src/ontology/HumanDO.obo\n",
    " \n",
    "*Nota*: https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/main/src/ontology/HumanDO.obo es el raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importación de lista de enfermedades \n",
    "\n",
    "# Se inicializa el diccionario y las variables (inicialmente vacias y false)\n",
    "disease_dict={}\n",
    "disease_key=False\n",
    "disease_name=[]\n",
    "\n",
    "# Se abre la página del texto raw, se pone todo en minúsculas y se aplica el decode\n",
    "obo=urllib.request.urlopen('https://raw.githubusercontent.com/DiseaseOntology/HumanDiseaseOntology/main/src/ontology/HumanDO.obo')\n",
    "obo=[line.lower().decode(\"utf-8\") for line in obo]\n",
    "\n",
    "# Para cada linea del texto:\n",
    "\n",
    "# Si empieza con [term] y NO hay definidos una key y value para el diccionario, se inicializan las variables.\n",
    "# Si empieza con [term] y SÍ hay definidos una key y value para el diccionario, se añaden al diccionario y se resetean\n",
    "\n",
    "# Si empieza con id, lo convierte en key (disease_key), quitando el \"id:\", el salto de línea y los espacios en blanco\n",
    "# Si empieza con name o synonym, lo añade a la lista de values (disease_name) tras eliminar los saltos de línea, el \n",
    "#     synonym, el exact [] en el caso de los synonyms, y las comillas.\n",
    "\n",
    "# Si empieza con [typedef], significa que ya no hay más terms, y que no interesa seguir parseando, con lo que se añade\n",
    "# el último key-value y se termina\n",
    "\n",
    "for line in obo:\n",
    "            \n",
    "    if line.startswith(\"[term]\"):\n",
    "        if disease_key and disease_name:\n",
    "            disease_dict[disease_key]=disease_name\n",
    "        disease_key=str()\n",
    "        disease_name=list()\n",
    "    \n",
    "    if line.startswith(\"id\"):\n",
    "        disease_key=line.strip(\"id:\").strip(\"\\n\").strip()\n",
    "\n",
    "    elif line.startswith(\"name\"):\n",
    "        disease_name.append(line.strip(\"name:\").strip(\"\\n\").strip())\n",
    "    \n",
    "    elif line.startswith(\"synonym\"):\n",
    "        disease_name.append(line.strip(\"\\n\").strip(\"synonym: \").strip( \"exact []\").replace('\"',\"\"))\n",
    "\n",
    "    elif line.startswith(\"[typedef]\"):\n",
    "        disease_dict[disease_key] = disease_name\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'El diccionario de enfermedades contiene {len(disease_dict)} entradas.')\n",
    "# El resultado es 13046 para la versión del 22 de diciembre de 2020"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Database para bacterias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**PLAN RAW PARA BACTERIAS**:  Se obtendrá una lista con el nombre científico de las bacterias, tanto con nombre genérico como epíteto específico (ojo con pasarla a  minúsculas) en un solo string. Una opción es generar un diccionario {key: Nombre científico completo pero todo junto y values: nombre científico separado y nombre abreviado}, y repetir el proceso anterior. Así, todos los nombres científicos (staphylococcus aureus, o s.aureus, hay que recordar que estará todo en minúscula) se cambiarán por el nombre sin espacios \n",
    "(staphylococcusaureus o staphylococcus_aureus, es sencillo de lograr de cualquier método) para que al tokenizar sea un solo token. \n",
    "\n",
    "He tenido que hacer una cosa un poco fea, poner en el github la lista de los taxids de bacterias (solamente de bacterias, obtenidos a través de un grep del categories.dmp del ftp del NCBI)\n",
    "\n",
    "Tarda mucho, quizás sería buena idea guardarlo en un archivo JSON?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Entrez.max_tries=5\n",
    "Entrez.email=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_dict={}\n",
    "\n",
    "dmp = urllib.request.urlopen(\"https://raw.githubusercontent.com/GuilleGorines/data/main/b_categories.dmp\")\n",
    "dmp = [line.decode(\"utf-8\").strip(\"\\n\").split(\"\\t\")[1] for line in dmp]\n",
    "dmp = set(dmp)\n",
    "\n",
    "print(f'La cantidad de especies bacterianas encontradas es {len(dmp)}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for num,organism in enumerate(dmp):\n",
    "    search = Entrez.efetch(db='taxonomy',id=organism)\n",
    "    result = Entrez.read(search)\n",
    "    result = dict(result[0])\n",
    "    \n",
    "    print(result[\"ScientificName\"].lower())\n",
    "    print(result[\"LineageEx\"][-1])\n",
    "    print(\"\\n\")\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generación del corpora con Entrez"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se establecen términos de búsqueda\n",
    "\n",
    "term = \"pokemon\"\n",
    "\n",
    "Entrez.max_tries=5\n",
    "Entrez.email=\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Búsqueda inicial para comprobar cantidad de coincidencias\n",
    "\n",
    "scout_search = Entrez.esearch(db=\"pubmed\", rettype=\"count\", term = term)\n",
    "scout_result = Entrez.read(scout_search)\n",
    "id_quantity = int(scout_result[\"Count\"])\n",
    "print(f\"{id_quantity} artículo(s) encontrados en PubMed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rondas de esearch dividido en rondas de longitud máxima (100k búsquedas)\n",
    "if id_quantity > 100000:\n",
    "    rounds = int(id_quantity/100000)+1\n",
    "    retmax=100*1000\n",
    "else:\n",
    "    rounds = 1\n",
    "    retmax = id_quantity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se inicializa la lista de abstracts \n",
    "retstart=0\n",
    "abstracts=[]\n",
    "\n",
    "# eFetch y adición de los resultados a la lista de abstracts. \n",
    "for round in range(0,rounds):\n",
    "    \n",
    "    search = Entrez.esearch(db=\"pubmed\", retmax=retmax, term=term)\n",
    "    \n",
    "    id_quantity -= retmax\n",
    "    retstart += retmax\n",
    "    if id_quantity < 100000:\n",
    "        retmax = id_quantity\n",
    "    result = Entrez.read(search)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for single_id in result['IdList']:\n",
    "    search = Entrez.efetch(db=\"pubmed\", id=single_id, rettype=\"medline\", retmode=\"text\")\n",
    "    record = list(Medline.parse(search))\n",
    "    record = dict(record[0])\n",
    "    try:\n",
    "        date = record[\"DP\"]\n",
    "        print(date)\n",
    "        record = record[\"AB\"]\n",
    "        abstracts.append([date,record])\n",
    "    except KeyError:\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cantidad_inicial=len(abstracts)\n",
    "\n",
    "print(f'La cantidad inicial de abstracts es {cantidad_inicial}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,j in enumerate(abstracts):\n",
    "    print(i,j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sustitución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bact_list = [bacteria for sublist in bact_dict.values() for bacteria in sublist]\n",
    "disease_list = [disease for sublist in disease_dict.values() for disease in sublist]\n",
    "\n",
    "bacterias_detectadas = []\n",
    "enfermedades_detectadas = []\n",
    "\n",
    "coincidencias = []\n",
    "\n",
    "for num,text in enumerate(abstracts):\n",
    "    bact_in_text = [num]\n",
    "    enf_in_text = [num]\n",
    "    \n",
    "    for bact in bact_list:\n",
    "        if bact in text:\n",
    "            taxid = key_from_value(bact_dict, bact)\n",
    "            text.replace(bact,taxid)\n",
    "            bact_in_text.append(taxid)\n",
    "    \n",
    "    for disease in disease_list:\n",
    "        if disease in text:\n",
    "            doid = key_from_value(disease_dict, disease)\n",
    "            text.replace(disease, doid)\n",
    "            enf_in_text.append(doid)\n",
    "    \n",
    "    bacterias_detectadas.append(bact_in_text)\n",
    "    enfermedades_detectadas.append(enf_in_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NUEVA PRUEBA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "import networkx as nx\n",
    "\n",
    "from Bio import Medline\n",
    "from Bio import Entrez\n",
    "\n",
    "import requests\n",
    "\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La cantidad de resultados es 2\n",
      "La cantidad de abstracts iniciales es 2\n"
     ]
    }
   ],
   "source": [
    "term = \"digimon\"\n",
    "Entrez.max_tries=5\n",
    "Entrez.email=\"\" # Necesario o error\n",
    "\n",
    "# Búsqueda inicial para comprobar cantidad de coincidencias\n",
    "\n",
    "scout_search = Entrez.esearch(db=\"pubmed\", rettype=\"count\", term = term)\n",
    "scout_result = Entrez.read(scout_search)\n",
    "id_quantity = int(scout_result[\"Count\"])\n",
    "print(f\"La cantidad de resultados es {id_quantity}\")\n",
    "\n",
    "# Rondas de esearch dividido en rondas de longitud máxima (100k búsquedas)\n",
    "if id_quantity > 100000:\n",
    "    rounds = round(id_quantity/100000)\n",
    "    retmax=100*1000\n",
    "else:\n",
    "    rounds = 1\n",
    "    retmax = id_quantity\n",
    "\n",
    "retstart=0\n",
    "abstracts=[]\n",
    "\n",
    "# eFetch y adición de los resultados a la lista de abstracts. \n",
    "for round in range(0,rounds):\n",
    "    \n",
    "    search = Entrez.esearch(db=\"pubmed\", retmax=retmax, term=term)\n",
    "    \n",
    "    id_quantity -= retmax\n",
    "    retstart += retmax\n",
    "    if id_quantity < 100000:\n",
    "        retmax = id_quantity\n",
    "    \n",
    "    result = Entrez.read(search)\n",
    "       \n",
    "for single_id in result['IdList']:\n",
    "    search = Entrez.efetch(db=\"pubmed\", id=single_id, rettype=\"medline\", retmode=\"text\")\n",
    "    record = list(Medline.parse(search))\n",
    "    record = dict(record[0])\n",
    "    try:\n",
    "        date = record[\"DP\"]\n",
    "        record = record[\"AB\"]\n",
    "        abstracts.append([date,record])\n",
    "    except KeyError:\n",
    "        pass\n",
    "\n",
    "cantidad_inicial=len(abstracts)\n",
    "\n",
    "print(f'La cantidad de abstracts iniciales es {cantidad_inicial}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2013 Feb 15\n",
      "1 The role of experience in the development of brain mechanisms for face recognition is intensely debated. Experience with subordinate- and individual-level classification of faces is thought, by some, to be foundational in the development of the specialization of face recognition. Studying children with extremely intense interests (EII) provides an opportunity to examine experience-related changes in non-face object recognition in a population where face expertise is not fully developed. Here, two groups of school-aged children -one group with an EII with Pokemon cards and another group of age-matched controls - underwent fMRI while viewing faces, Pokemon characters, Pokemon objects, and Digimon characters. Pokemon objects were non-character Pokemon cards that experts do not typically individuate during game play and trading. Neither experts nor controls had previous experience with Digimon characters. As expected, experts and controls showed equivalent activation in the fusiform face area (FFA) with face stimuli. As predicted by the expertise hypothesis, experts showed greater activation than controls with Pokemon characters, and showed greater activation with Pokemon characters than Pokemon objects. Experts and controls showed equivalent activation with Digimon characters. However, heightened activation with Digimon characters in both groups suggested that there are other strong influences on the activation of the FFA beyond stimulus characteristics, experience, and classification level. By demonstrating the important role of expertise, the findings are inconsistent with a purely face-specific account of FFA function. To our knowledge, this is the first demonstration of the effects of expertise and categorization level on activation in the FFA in a group of typically developing children.\n"
     ]
    }
   ],
   "source": [
    "for num, abstract in enumerate(abstracts):\n",
    "    print(num,abstract)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
